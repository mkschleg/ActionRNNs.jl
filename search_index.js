var documenterSearchIndex = {"docs":
[{"location":"experiments/image_dir_tmaze/#Image-Directional-TMaze","page":"Image Directional TMaze","title":"Image Directional TMaze","text":"","category":"section"},{"location":"experiments/image_dir_tmaze/","page":"Image Directional TMaze","title":"Image Directional TMaze","text":"See the experiment file for more details. This follows the old system, and wasn't converted to the new experiment layout. Should be more-or-less the same as the other experiments.","category":"page"},{"location":"experiments/ringworld/#RingWorld","page":"RingWorld","title":"RingWorld","text":"","category":"section"},{"location":"experiments/ringworld/#Experience-Replay-Experiment","page":"RingWorld","title":"Experience Replay Experiment","text":"","category":"section"},{"location":"experiments/ringworld/","page":"RingWorld","title":"RingWorld","text":"RingWorldERExperiment\nRingWorldERExperiment.main_experiment\nRingWorldERExperiment.working_experiment\nRingWorldERExperiment.default_config\nRingWorldERExperiment.get_ann_size\nRingWorldERExperiment.construct_agent\nRingWorldERExperiment.construct_env","category":"page"},{"location":"experiments/ringworld/#RingWorldERExperiment","page":"RingWorld","title":"RingWorldERExperiment","text":"RingWorldERExperiment\n\nThe experimental module used to run experiments in RingWorld for the ER Agents. \n\n\n\n\n\n","category":"module"},{"location":"experiments/ringworld/#RingWorldERExperiment.main_experiment","page":"RingWorld","title":"RingWorldERExperiment.main_experiment","text":"main_experiment\n\nRun an experiment from config. See RingWorldERExperiment.working_experiment  for details on running on the command line and RingWorldERExperiment.default_config  for info about the default configuration.\n\n\n\n\n\n","category":"function"},{"location":"experiments/ringworld/#RingWorldERExperiment.working_experiment","page":"RingWorld","title":"RingWorldERExperiment.working_experiment","text":"working_experiment\n\nCreates a wrapper experiment where the main experiment is called with progress=true, testing=true  and the config is the default_config with the addition of the keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"experiments/ringworld/#RingWorldERExperiment.default_config","page":"RingWorld","title":"RingWorldERExperiment.default_config","text":"Automatically generated docs for RingWorldERExperiment config.\n\nExperiment details.\n\n\n\nseed::Int: seed of RNG\nsteps::Int: Number of steps taken in the experiment\nsynopsis::Bool: Report full results or a synopsis.\n\nLogging Extras\n\nBy default the experiment will log and save (depending on the synopsis flag) the logging group :EXP.  You can add extra logging groups and [group, name] pairs using the below arguments. Everything  added to save_extras will be passed to the save operation, and will be logged automatically. The  groups and names added to log_extras will be ommited from save_results but still passed back to the user through the data dict.\n\n<log_extras::Vector{Union{String, Vector{String}}>: which group and <name> to log to the data dict. This will not be passed to save.\n<save_extras::Vector{Union{String, Vector{String}}>: which groups and <names> to log to the data dict. This will be passed to save.\n\nEnvironment details\n\nThis experiment uses the RingWorld environment. The usable args are:\n\nsize::Int: Number of states in the ring.\n\nagent details\n\nRNN\n\nThe RNN used for this experiment and its total hidden size,  as well as a flag to use (or not use) zhu's deep  action network. See \n\ncell::String: The typeof cell. Many types are possible.\ndeepaction::Bool: Whether to use Zhu et. al.'s deep action 4 RNNs idea.   -internal_a::Int: the size of the action representation layer when deepaction=true\nnumhidden::Int:  Size of hidden state in RNNs.\n\nPrediction Problem\n\nDefine the prediction problem for the experiment.\n\nouthorde::String: The horde used to construct the targets.\noutgamma::Float64: The discount (used by specific hordes).\n\nOptimizer details\n\nFlux optimizers are used. See flux documentation and ExpUtils.Flux.get_optimizer for details.\n\nopt::String: The name of the optimizer used\nParameters defined by the particular optimizer.\n\nLearning update and replay details including:\n\nReplay: \nreplay_size::Int: How many transitions are stored in the replay.\nwarm_up::Int: How many steps for warm-up (i.e. before learning begins).\nUpdate details: \nbatch_size::Int: size of batch\ntruncation::Int: Length of sequences used for training.\nupdate_freq::Int: Time between updates (counted in agent interactions)\ntarget_update_freq::Int: Time between target network updates (counted in agent interactions)\nhs_learnable::Bool: Strategy for dealing w/ hidden state in buffer.\n\nDefault Performance:\n\nTime: 0:00:56\nDict{String, Matrix{Float32}} with 2 entries:\n  \\\"err\\\"  => [0.0 0.0; 0.0 0.0; … ; 0.0128746 -0.000129551; 0.00117147 -0.00140008]\n  \\\"pred\\\" => [0.0 0.0; 0.0 0.0; … ; 0.0128746 -0.000129551; 1.00117 -0.00140008]\n\n\n\n\n\n","category":"function"},{"location":"experiments/ringworld/#RingWorldERExperiment.get_ann_size","page":"RingWorld","title":"RingWorldERExperiment.get_ann_size","text":"get_ann_size\n\nHelper function which constructs the environment and agent using default config and kwargs then returns the number of parameters in the model.\n\n\n\n\n\n","category":"function"},{"location":"experiments/ringworld/#RingWorldERExperiment.construct_agent","page":"RingWorld","title":"RingWorldERExperiment.construct_agent","text":"construct_agent\n\nConstruct the agent for the ringworld experiments.\n\n\n\n\n\n","category":"function"},{"location":"experiments/ringworld/#RingWorldERExperiment.construct_env","page":"RingWorld","title":"RingWorldERExperiment.construct_env","text":"construct_env\n\nConstruct ringworld environment with size::Int.\n\n\n\n\n\n","category":"function"},{"location":"experiments/#Experiments","page":"Experiments","title":"Experiments","text":"","category":"section"},{"location":"experiments/","page":"Experiments","title":"Experiments","text":"There are many experiments run in the paper associated with this paper. All the experiments are run using [Reproduce.jl][https://github.com/mkschleg/Reproduce.jl]. To run a particular config (see the configs and final_runs folders).","category":"page"},{"location":"experiments/","page":"Experiments","title":"Experiments","text":"To run a specific config on your own machine from the ActionRNNs.jl root folder: ","category":"page"},{"location":"experiments/","page":"Experiments","title":"Experiments","text":"julia --project parallel/toml_parallel.jl <<config>>","category":"page"},{"location":"experiments/","page":"Experiments","title":"Experiments","text":"configs: This includes configs for all sweeps run in this empirical analysis. If a config doesn't run \nfinal_runs: This includes all the configs and hyperparameters for gathering the final runs after the hyperparameter sweep. ","category":"page"},{"location":"experiments/","page":"Experiments","title":"Experiments","text":"There are also various pluto notebooks for analyzing data, and plotting. We don't provide details on how to use these notebooks, but provide them. You can also find the experiments used to construct tsne plots and various other useful scripts in the scripts folder","category":"page"},{"location":"experiments/tmaze/#TMaze","page":"TMaze","title":"TMaze","text":"","category":"section"},{"location":"experiments/tmaze/#Experience-Replay-Agent","page":"TMaze","title":"Experience Replay Agent","text":"","category":"section"},{"location":"experiments/tmaze/","page":"TMaze","title":"TMaze","text":"TMazeERExperiment\nTMazeERExperiment.main_experiment\nTMazeERExperiment.working_experiment\nTMazeERExperiment.default_config\nTMazeERExperiment.get_ann_size\nTMazeERExperiment.construct_agent\nTMazeERExperiment.construct_env","category":"page"},{"location":"experiments/tmaze/#TMazeERExperiment","page":"TMaze","title":"TMazeERExperiment","text":"TMazeERExperiment\n\nThe experimental module for the TMaze experiments.\n\n\n\n\n\n","category":"module"},{"location":"experiments/tmaze/#TMazeERExperiment.main_experiment","page":"TMaze","title":"TMazeERExperiment.main_experiment","text":"main_experiment\n\nThis is the main experiment function for TMaze ER Agents. See TMazeERExperiment.working_experiment  for details on running on the command line and TMazeERExperiment.default_config  for info about the default configuration.\n\n\n\n\n\n","category":"function"},{"location":"experiments/tmaze/#TMazeERExperiment.working_experiment","page":"TMaze","title":"TMazeERExperiment.working_experiment","text":"working_experiment\n\nCreates a wrapper experiment where the main experiment is called with progress=true, testing=true  and the config is the default_config with the addition of the keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"experiments/tmaze/#TMazeERExperiment.default_config","page":"TMaze","title":"TMazeERExperiment.default_config","text":"Automatically generated docs for TMazeERExperiment config.\n\nExperiment details.\n\nseed::Int: seed of RNG\nsteps::Int: Number of steps taken in the experiment\n\nEnvironment details\n\nThis experiment uses the TMaze environment. The usable args are:\n\nsize::Int: Size of the hallway in tmaze.\n\nagent details\n\nRNN\n\nThe RNN used for this experiment and its total hidden size,  as well as a flag to use (or not use) zhu's deep  action network. See \n\ncell::String: The typeof cell. Many types are possible.\ndeepaction::Bool: Whether to use Zhu et. al.'s deep action 4 RNNs idea.   -internal_a::Int: the size of the action representation layer when deepaction=true\nnumhidden::Int:  Size of hidden state in RNNs.\n\nOptimizer details\n\nFlux optimizers are used. See flux documentation and ExpUtils.Flux.get_optimizer for details.\n\nopt::String: The name of the optimizer used\nParameters defined by the particular optimizer.\n\nLearning update and replay details including:\n\nReplay: \nreplay_size::Int: How many transitions are stored in the replay.\nwarm_up::Int: How many steps for warm-up (i.e. before learning begins).\nUpdate details: \nlupdate::String: Learning update name\ngamma::Float: the discount for learning update.\nbatch_size::Int: size of batch\ntruncation::Int: Length of sequences used for training.\nupdate_wait::Int: Time between updates (counted in agent interactions)\ntarget_update_wait::Int: Time between target network updates (counted in agent interactions)\nhs_strategy::String: Strategy for dealing w/ hidden state in buffer.\n\nDefault performance:\n\nTime: 0:01:19\n  episode:    6455\n  successes:  0.9600399600399601\n  loss:       0.990142\n  l1:         0.000502145\n  action:     2\n  preds:      Float32[0.3016336, 3.6225605, -2.5592222, 1.884988]\n  grad:       0.0\n\n\n\n\n\n","category":"function"},{"location":"experiments/tmaze/#TMazeERExperiment.get_ann_size","page":"TMaze","title":"TMazeERExperiment.get_ann_size","text":"get_ann_size\n\nHelper function which constructs the environment and agent using default config and kwargs then returns the number of parameters in the model.\n\n\n\n\n\n","category":"function"},{"location":"experiments/tmaze/#TMazeERExperiment.construct_agent","page":"TMaze","title":"TMazeERExperiment.construct_agent","text":"construct_agent\n\nConstruct the agent for TMazeERExperiment. \n\n\n\n\n\n","category":"function"},{"location":"experiments/tmaze/#TMazeERExperiment.construct_env","page":"TMaze","title":"TMazeERExperiment.construct_env","text":"construct_env\n\nConstruct direction tmaze using:\n\nsize::Int size of hallway.\n\n\n\n\n\n","category":"function"},{"location":"library/#General-Documentation","page":"General Documentation","title":"General Documentation","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"This page hosts the general documentation of the ActionRNNs.jl library. This includes all research code used in this project.","category":"page"},{"location":"library/#Contents","page":"General Documentation","title":"Contents","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"Pages = [\"library.md\"]","category":"page"},{"location":"library/#Index","page":"General Documentation","title":"Index","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"Pages = [\"library.md\"]","category":"page"},{"location":"library/#Cells","page":"General Documentation","title":"Cells","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.AbstractActionRNN\nActionRNNs._needs_action_input","category":"page"},{"location":"library/#ActionRNNs.AbstractActionRNN","page":"General Documentation","title":"ActionRNNs.AbstractActionRNN","text":"AbstractActionRNN\n\nAn abstract struct which will take the current hidden state and  a tuple of observations and actions and returns the next hidden state.\n\n\n\n\n\n","category":"type"},{"location":"library/#ActionRNNs._needs_action_input","page":"General Documentation","title":"ActionRNNs._needs_action_input","text":"_needs_action_input\n\nIf true, this means the cell or layer needs a tuple as input.\n\n\n\n\n\n","category":"function"},{"location":"library/#Basic-Cells","page":"General Documentation","title":"Basic Cells","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.AARNN\nActionRNNs.AAGRU\nActionRNNs.AALSTM\nActionRNNs.MARNN\nActionRNNs.MAGRU\nActionRNNs.MALSTM\nActionRNNs.FacMARNN\nActionRNNs.FacMAGRU\nActionRNNs.FacTucMARNN\nActionRNNs.FacTucMAGRU","category":"page"},{"location":"library/#ActionRNNs.AARNN","page":"General Documentation","title":"ActionRNNs.AARNN","text":"AARNN(in::Integer, actions::Integer, out::Integer, σ = tanh)\n\nLike an RNN cell, except takes a tuple (action, observation) as input. The action is used with get_waa with results added to the usual update.\n\nThe update is as follows:     σ.(Wi*o .+ get_waa(Wa, a) .+ Wh*h .+ b)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.AAGRU","page":"General Documentation","title":"ActionRNNs.AAGRU","text":"AAGRU(in, actions, out)\n\nAdditive Action Gated Recurrent Unit layer. Behaves like an AARNN but uses a GRU internal structure\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.AALSTM","page":"General Documentation","title":"ActionRNNs.AALSTM","text":"AALSTM(in::Integer, na::Integer, out::Integer)\n\nAdditive Action Long Short Term Memory recurrent layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. See this article for a good overview of the internals.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.MARNN","page":"General Documentation","title":"ActionRNNs.MARNN","text":"MARNN(in::Integer, actions::Integer, out::Integer, σ = tanh)\n\nThis cell incorporates the action as a multiplicative operation. We use  contract_WA and get_waa to handle this.\n\nThe update is as follows:\n\nnew_h = σ.(contract_WA(m.Wx, a, o) .+ contract_WA(m.Wh, a, h) .+ get_waa(m.b, a))\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.MAGRU","page":"General Documentation","title":"ActionRNNs.MAGRU","text":"MAGRU(in, actions, out)\n\nMultiplicative Action Gated Recurrent Unit layer. Behaves like an MARNN but uses a GRU internal structure.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.MALSTM","page":"General Documentation","title":"ActionRNNs.MALSTM","text":"MALSTM(in::Integer, na::Integer, out::Integer)\n\nMuliplicative Action Long Short Term Memory recurrent layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. See this article for a good overview of the internals.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.FacMARNN","page":"General Documentation","title":"ActionRNNs.FacMARNN","text":"FacMARNN(in::Integer, actions::Integer, out::Integer, factors, σ = tanh; init_style=\"ignore\")\n\nThis cell incorporates the action as a multiplicative operation, but as a factored approximation of the multiplicative version. This cell uses get_waa. Uses CP decomposition.\n\nThe update is as follows:\n\n   new_h = m.σ.(W*((Wx*o .+ Wh*h) .* get_waa(Wa, a)) .+ get_waa(m.b, a))\n\nThree init_styles:\n\nstandard: using init and initb w/o any keywords\nignore: W = init(out, factors, ignore_dims=2)\ntensor: Decompose W_t = init(actions, out, in+out; ignore_dims=1) to get W_o, W_a, W_hi using TensorToolbox.cp_als.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.FacMAGRU","page":"General Documentation","title":"ActionRNNs.FacMAGRU","text":"FacMAGRU(in, actions, out, factors)\n\nFactored Multiplicative Action Gated Recurrent Unit layer. Behaves like an FacMARNN but uses a GRU internal structure.\n\nThree init_styles:\n\nstandard: using init and initb w/o any keywords\nignore: W = init(out, factors, ignore_dims=2)\ntensor: Decompose W_t = init(actions, out, in+out; ignore_dims=1) to get W_o, W_a, W_hi using TensorToolbox.cp_als.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.FacTucMARNN","page":"General Documentation","title":"ActionRNNs.FacTucMARNN","text":"FacTucMARNN(in::Integer, actions::Integer, out::Integer, action_factors, out_factors, in_factors, σ = tanh; init_style=\"ignore\")\n\nThis cell incorporates the action as a multiplicative operation, but as a factored approximation of the multiplicative version. This cell uses get_waa. Uses Tucker decomposition.\n\nThree init_styles:\n\nstandard: using init and initb w/o any keywords\nignore: Wa = init(action_factors, actions; ignore_dims=2)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.FacTucMAGRU","page":"General Documentation","title":"ActionRNNs.FacTucMAGRU","text":"FacTucMAGRU(in, actions, out, factors)\n\nFactored Multiplicative Action Gated Recurrent Unit layer. Behaves like an FacTucMARNN but uses a GRU internal structure.\n\n\n\n\n\n","category":"function"},{"location":"library/#Combo-Cells","page":"General Documentation","title":"Combo Cells","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.CaddRNN\nActionRNNs.CaddGRU\nActionRNNs.CaddAAGRU\nActionRNNs.CaddMAGRU\nActionRNNs.CaddElRNN\nActionRNNs.CaddElGRU\nActionRNNs.CcatRNN\nActionRNNs.CcatGRU\nActionRNNs.CsoftmaxElRNN\nActionRNNs.CsoftmaxElGRU","category":"page"},{"location":"library/#ActionRNNs.CaddRNN","page":"General Documentation","title":"ActionRNNs.CaddRNN","text":"CaddRNN(in, actions, out, σ = tanh)\n\nMixing between AARNN and MARNN through a weighting\n\nh′ = (w[1]*new_hAA + w[2]*new_hMA) ./ sum(w)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.CaddGRU","page":"General Documentation","title":"ActionRNNs.CaddGRU","text":"CaddGRU(in, actions, out, σ = tanh)\n\nMixing between AAGRU and MAGRU through a weighting\n\nh′ = (w[1]*new_hAA + w[2]*new_hMA) ./ sum(w)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.CaddAAGRU","page":"General Documentation","title":"ActionRNNs.CaddAAGRU","text":"CaddAAGRU(in, actions, out)\n\nMixing between two AAGRU cells through weighting\n\n```julia h′ = (w[1]new_hAA1 + w[2]new_hAA2) ./ sum(w)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.CaddMAGRU","page":"General Documentation","title":"ActionRNNs.CaddMAGRU","text":"CaddMAGRU(in, actions, out)\n\nMixing between two MAGRU cells through weighting\n\n```julia h′ = (w[1]new_hMA1 + w[2]new_hMA2) ./ sum(w)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.CaddElRNN","page":"General Documentation","title":"ActionRNNs.CaddElRNN","text":"CaddElRNN(in, actions, out, σ = tanh)\n\nMixing between AARNN and MARNN through a weighting\n\nh′ = (AA_θ .* AA_h′ .+ MA_θ .* MA_h′) ./ (AA_θ .+ MA_θ)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.CaddElGRU","page":"General Documentation","title":"ActionRNNs.CaddElGRU","text":"CaddElGRU(in, actions, out)\n\nMixing between AAGRU and MAGRU through a weighting\n\nh′ = (AA_θ .* AA_h′ .+ MA_θ .* MA_h′) ./ (AA_θ .+ MA_θ)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.CcatRNN","page":"General Documentation","title":"ActionRNNs.CcatRNN","text":"CcatRNN(in, actions, out, σ = tanh)\n\nMixing between AARNN and MARNN through\n\nh′ = cat(AA_h′, MA_h′)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.CcatGRU","page":"General Documentation","title":"ActionRNNs.CcatGRU","text":"CcatRNN(in, actions, out)\n\nMixing between AAGRU and MAGRU through\n\nh′ = cat(AA_h′, MA_h′)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.CsoftmaxElRNN","page":"General Documentation","title":"ActionRNNs.CsoftmaxElRNN","text":"CaddElRNN(in, actions, out, σ = tanh)\n\nMixing between AARNN and MARNN through a weighting\n\nh′ = (AA_θ .* AA_h′ .+ MA_θ .* MA_h′) ./ (AA_θ .+ MA_θ)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.CsoftmaxElGRU","page":"General Documentation","title":"ActionRNNs.CsoftmaxElGRU","text":"CaddElGRU(in, actions, out)\n\nMixing between AAGRU and MAGRU through a weighting\n\nh′ = (AA_θ .* AA_h′ .+ MA_θ .* MA_h′) ./ (AA_θ .+ MA_θ)\n\n\n\n\n\n","category":"function"},{"location":"library/#Mixed-Cells","page":"General Documentation","title":"Mixed Cells","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.MixRNN\nActionRNNs.MixElRNN\nActionRNNs.MixGRU\nActionRNNs.MixElGRU\nActionRNNs.ActionGatedRNN","category":"page"},{"location":"library/#ActionRNNs.MixRNN","page":"General Documentation","title":"ActionRNNs.MixRNN","text":"MixRNN(in, actions, out, num_experts, σ = tanh)\n\nMixing between num_experts AARNN cells. Uses the weighting\n\nh′ = sum(θ[i] .* expert_h′[i] for i in 1:length(θ)) ./ sum(θ)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.MixElRNN","page":"General Documentation","title":"ActionRNNs.MixElRNN","text":"MixElRNN(in, actions, out, num_experts, σ = tanh)\n\nMixing between num_experts AARNN cells. Uses the weighting\n\nh′ = sum(θ[i] .* expert_h′[i] for i in 1:length(θ)) ./ sum(θ)\n\n(here θ[i] is a vector).\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.MixGRU","page":"General Documentation","title":"ActionRNNs.MixGRU","text":"MixGRU(in, actions, out, num_experts)\n\nMixing between num_experts AAGRU cells. Uses the weighting\n\nh′ = sum(θ[i] .* expert_h′[i] for i in 1:length(θ)) ./ sum(θ)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.MixElGRU","page":"General Documentation","title":"ActionRNNs.MixElGRU","text":"MixElGRU(in, actions, out, num_experts)\n\nMixing between num_experts AAGRU cells. Uses the weighting\n\nh′ = sum(θ[i] .* expert_h′[i] for i in 1:length(θ)) ./ sum(θ)\n\n(here θ[i] is a vector).\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.ActionGatedRNN","page":"General Documentation","title":"ActionRNNs.ActionGatedRNN","text":"ActionGatedRNN(in::Integer, na, internal, out::Integer, σ = tanh)\n\nThe most basic recurrent layer; essentially acts as a Dense layer, but with the output fed back into the input each time step.\n\n\n\n\n\n","category":"function"},{"location":"library/#Old/DefunctCells","page":"General Documentation","title":"Old/DefunctCells","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.GAUGRU\nActionRNNs.GAIGRU\nActionRNNs.GAIARNN\nActionRNNs.GAIAGRU\nActionRNNs.GAIALSTM","category":"page"},{"location":"library/#ActionRNNs.GAUGRU","page":"General Documentation","title":"ActionRNNs.GAUGRU","text":"GAUGRU(in::Integer, na::Integer, internal::Integer, out::Integer)\n\nGated Action Input Gated Recurrent Unit layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. See this article for a good overview of the internals.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.GAIGRU","page":"General Documentation","title":"ActionRNNs.GAIGRU","text":"GAIGRU(in::Integer, na::Integer, internal::Integer, out::Integer)\n\nGated Action Input Gated Recurrent Unit layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. See this article for a good overview of the internals.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.GAIARNN","page":"General Documentation","title":"ActionRNNs.GAIARNN","text":"GAIARNN(in::Integer, na, internal, out::Integer, σ = tanh)\n\nThe most basic recurrent layer; essentially acts as a Dense layer, but with the output fed back into the input each time step.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.GAIAGRU","page":"General Documentation","title":"ActionRNNs.GAIAGRU","text":"GAIAGRU(in::Integer, na::Integer, internal::Integer, out::Integer)\n\nGated Action Input Gated Recurrent Unit layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. See this article for a good overview of the internals.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.GAIALSTM","page":"General Documentation","title":"ActionRNNs.GAIALSTM","text":"GAIALSTM(in::Integer, na::Integer, out::Integer)\n\nGated Action Input by Action Long Short Term Memory recurrent layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. See this article for a good overview of the internals.\n\n\n\n\n\n","category":"function"},{"location":"library/#Shared-operations-for-cells","page":"General Documentation","title":"Shared operations for cells","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"HelpfulKernelFuncs.contract_WA\nHelpfulKernelFuncs.get_waa","category":"page"},{"location":"library/#HelpfulKernelFuncs.contract_WA","page":"General Documentation","title":"HelpfulKernelFuncs.contract_WA","text":"contract_WA(W, a::Int, x)\ncontract_WA(W, a::AbstractVector{Int}, x)\ncontract_WA(W, a::AbstractVector{<:AbstractFloat}, x)\ncontract_WA(W::CuArray, a::AbstractVector{Int}, x)\n\nThis contraction operator will take the weights W, action (or action vector for batches) a, and features. The weight matrix is assumed to be in nactions × out × in.\n\n\n\n\n\n","category":"function"},{"location":"library/#HelpfulKernelFuncs.get_waa","page":"General Documentation","title":"HelpfulKernelFuncs.get_waa","text":"get_waa(Wa, a)\n\nDifferent ways of handeling geting action value from a set of weights. This operation can be seen as Wa*a where Wa is the weight matrix, and a is the action representation. This is to be used with various cells to incorporate this operation more reliably.\n\n\n\n\n\n","category":"function"},{"location":"library/#Other-Layers","page":"General Documentation","title":"Other Layers","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.ActionDense","category":"page"},{"location":"library/#ActionRNNs.ActionDense","page":"General Documentation","title":"ActionRNNs.ActionDense","text":"ActionDense(in, na, out, σ; init, bias)\n\nCreate an actions Dense layer. This layer takes in a tuple (action, observaiton) and returns the dense layer using and additive approach. This can be used for previous actions or current actions.\n\n\n\n\n\n","category":"type"},{"location":"library/#Learning-Updates","page":"General Documentation","title":"Learning Updates","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.QLearning","category":"page"},{"location":"library/#ActionRNNs.QLearning","page":"General Documentation","title":"ActionRNNs.QLearning","text":"QLearning\nQLearningMSE(γ)\nQLearningSUM(γ)\nQLearningHUBER(γ)\n\nWatkins q-learning with various loss functions.\n\n\n\n\n\n","category":"type"},{"location":"library/#Constructors","page":"General Documentation","title":"Constructors","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.build_rnn_layer\nActionRNNs.build_gating_network","category":"page"},{"location":"library/#ActionRNNs.build_rnn_layer","page":"General Documentation","title":"ActionRNNs.build_rnn_layer","text":"build_rnn_layer(in, actions, out, parsed, rng)\n\nBuild an rnn layer according from parsed. This assumes the \"cell\" key is in the parsed dict. in, actions, and out are integers. must explicitly pass in a RNG.\n\nGets layer constructor from either the ActionRNNs or Flux namespaces.\n\nTypes of build types\n\nBuildActionRNN: AARNN, MARNN, AAGRU, MAGRU, AALSTM, MALSTM\nBuildFactored: FacMARNN, FacMAGRU\nBuildTucFactored: FacTucMARNN, FacTucMAGRU\nBuildComboCat: CcatRNN, CcatGRU\nBuildComboAdd: CaddRNN, CaddGRU, CaddAAGRU, CaddMAGRU, CaddElRNN\nBuildMixed: MixRNN, MixElRNN, MixElGRU, MixGRU\n\n\n\n\n\nbuild_rnn_layer(::BuildActionRNN, args...; kwargs...)\n\nStandard Additive and Multiplicative cells. No extra parameters.\n\n\n\n\n\nbuild_rnn_layer(::BuildFactored, args...; kwargs...)\n\nFactored (not tucker) cells. Extra Config Options:\n\ninit_style::String: They style of init. Check your cell for possible options.\nfactors::Int: Number of factors in factorization.\n\n\n\n\n\nbuild_rnn_layer(::BuildTucFactored, args...; kwargs...)\n\nTucker Factored cells: Extra Config Options:\n\nin_factors::Int: Number of factors in input matrix\naction_factors::Int: Number of factors in action matrix\nout_factors::Int: Number of factors in out matrix\n\n\n\n\n\nbuild_rnn_layer(::BuildComboCat, args...; kwargs...)\n\nCombo cat AA/MA cells. No Extra Params.\n\n\n\n\n\nbuild_rnn_layer(::BuildComboAdd, args...; kwargs...)\n\nCombo add AA/MA cells. No Extra Params.\n\n\n\n\n\nbuild_rnn_layer(::BuildMixed, args...; kwargs...)\n\nMixed layers. Extra Config Options -num_experts::Int: number of parallel cells in mixture.\n\n\n\n\n\nbuild_rnn_layer(::BuildFlux, args...; kwargs...)\n\nFlux cell. No extra parameters.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.build_gating_network","page":"General Documentation","title":"ActionRNNs.build_gating_network","text":"build_gating_network\n\n[[out, activation]]\n\n\n\n\n\n","category":"function"},{"location":"library/#Agents","page":"General Documentation","title":"Agents","text":"","category":"section"},{"location":"library/#Experience-Replay-Agents","page":"General Documentation","title":"Experience Replay Agents","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.AbstractERAgent","category":"page"},{"location":"library/#ActionRNNs.AbstractERAgent","page":"General Documentation","title":"ActionRNNs.AbstractERAgent","text":"AbstractERAgent\n\nThe abstract struct for building experience replay agents.\n\nexample agent: mutable struct DRQNAgent{ER, Φ,  Π, HS<:AbstractMatrix{Float32}} <: AbstractERAgent     lu::LearningUpdate     opt::O     model::C     target_network::CT\n\nbuild_features::F\nstate_list::DataStructures.CircularBuffer{Φ}\n\nhidden_state_init::Dict{Symbol, HS}\n\nreplay::ER\nupdate_timer::UpdateTimer\ntarget_update_timer::UpdateTimer\n\nbatch_size::Int\nτ::Int\n\ns_t::Φ\nπ::Π\nγ::Float32\n\naction::Int\nam1::Int\naction_prob::Float64\n\nhs_learnable::Bool\nbeg::Bool\ncur_step::Int\n\nhs_tr_init::Dict{Symbol, HS}\n\nend\n\n\n\n\n\n","category":"type"},{"location":"library/#Instantiations","page":"General Documentation","title":"Instantiations","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.DRQNAgent\nActionRNNs.DRTDNAgent","category":"page"},{"location":"library/#ActionRNNs.DRQNAgent","page":"General Documentation","title":"ActionRNNs.DRQNAgent","text":"DRQNAgent\n\nAn intense function... lol.\n\n\n\n\n\n","category":"type"},{"location":"library/#ActionRNNs.DRTDNAgent","page":"General Documentation","title":"ActionRNNs.DRTDNAgent","text":"Basic DRQNAgent.\n\n\n\n\n\n","category":"type"},{"location":"library/#Implementation-details","page":"General Documentation","title":"Implementation details","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.get_replay\nActionRNNs.get_learning_update\nActionRNNs.get_device\nActionRNNs.get_action_and_prob\nActionRNNs.get_model\nActionRNNs.MinimalRLCore.start!(agent::ActionRNNs.AbstractERAgent, s, rng; kwargs...)\nActionRNNs.MinimalRLCore.step!(agent::ActionRNNs.AbstractERAgent, env_s_tp1, r, terminal, rng; kwargs...)\nActionRNNs.MinimalRLCore.step!\nActionRNNs.training_mode\nActionRNNs.set_training_mode!\nActionRNNs.update!(agent::ActionRNNs.AbstractERAgent{<:ActionRNNs.ControlUpdate}, rng)\nActionRNNs.update!(agent::ActionRNNs.AbstractERAgent{<:ActionRNNs.PredictionUpdate}, rng)\nActionRNNs.update!\nActionRNNs.update_target_network!","category":"page"},{"location":"library/#ActionRNNs.get_replay","page":"General Documentation","title":"ActionRNNs.get_replay","text":"get_replay(agent::AbstractERAgent)\n\nGet the replay buffer from the agent.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.get_learning_update","page":"General Documentation","title":"ActionRNNs.get_learning_update","text":"get_learning_update(agent::AbstractERAgent)\n\nGet the learning update from the agent.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.get_device","page":"General Documentation","title":"ActionRNNs.get_device","text":"get_device(agent::AbstractERAgent)\n\nGet the current device from the agent.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.get_action_and_prob","page":"General Documentation","title":"ActionRNNs.get_action_and_prob","text":"get_action_and_prob(π, values, rng)\n\nGet action and the associated probability of taking the action.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.get_model","page":"General Documentation","title":"ActionRNNs.get_model","text":"get_model(agent::AbstractERAgent)\n\nreturn the model from the agent.\n\n\n\n\n\n","category":"function"},{"location":"library/#MinimalRLCore.start!-Tuple{ActionRNNs.AbstractERAgent, Any, Any}","page":"General Documentation","title":"MinimalRLCore.start!","text":"    MinimalRLCore.start!(agent::AbstractERAgent, s, rng; kwargs...)\n\nStart the agent for a new episode. \n\n\n\n\n\n","category":"method"},{"location":"library/#MinimalRLCore.step!-Tuple{ActionRNNs.AbstractERAgent, Any, Any, Any, Any}","page":"General Documentation","title":"MinimalRLCore.step!","text":"MinimalRLCore.step!(agent::AbstractERAgent, env_s_tp1, r, terminal, rng; kwargs...)\n\nstep! for an experience replay agent.\n\n\n\n\n\n","category":"method"},{"location":"library/#MinimalRLCore.step!","page":"General Documentation","title":"MinimalRLCore.step!","text":"MinimalRLCore.step!(agent::AbstractERAgent, env_s_tp1, r, terminal, rng; kwargs...)\n\nstep! for an experience replay agent.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.training_mode","page":"General Documentation","title":"ActionRNNs.training_mode","text":"training_mode(agent::AbstractERAgent)\n\nreturns bool whether the agent is in training mode.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.set_training_mode!","page":"General Documentation","title":"ActionRNNs.set_training_mode!","text":"set_training_mode(agent::AbstractERAgent, mode::Bool)\n\nsets training mode to boolean value\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.update!-Tuple{ActionRNNs.AbstractERAgent{<:ActionRNNs.ControlUpdate}, Any}","page":"General Documentation","title":"ActionRNNs.update!","text":"update!(agent::AbstractERAgent{<:ControlUpdate}, rng)\n\nUpdate the parameters of the model.\n\n\n\n\n\n","category":"method"},{"location":"library/#ActionRNNs.update!-Tuple{ActionRNNs.AbstractERAgent{<:ActionRNNs.PredictionUpdate}, Any}","page":"General Documentation","title":"ActionRNNs.update!","text":"update!(agent::AbstractERAgent{<:PredictionUpdate}, rng)\n\nUpdate the parameters of the model.\n\n\n\n\n\n","category":"method"},{"location":"library/#ActionRNNs.update!","page":"General Documentation","title":"ActionRNNs.update!","text":"update!(agent::AbstractERAgent{<:ControlUpdate}, rng)\n\nUpdate the parameters of the model.\n\n\n\n\n\nupdate!(agent::AbstractERAgent{<:PredictionUpdate}, rng)\n\nUpdate the parameters of the model.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.update_target_network!","page":"General Documentation","title":"ActionRNNs.update_target_network!","text":"update_target_network!\n\nUpdate the target network.\n\n\n\n\n\n","category":"function"},{"location":"library/#Online-Agents","page":"General Documentation","title":"Online Agents","text":"","category":"section"},{"location":"library/#Tools/Utils","page":"General Documentation","title":"Tools/Utils","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.UpdateTimer\nActionRNNs.make_obs_list\nActionRNNs.build_new_feat","category":"page"},{"location":"library/#ActionRNNs.UpdateTimer","page":"General Documentation","title":"ActionRNNs.UpdateTimer","text":"UpdateTimer\n\nKeeps track of timer for doing things in the agent.\n\n\n\n\n\n","category":"type"},{"location":"library/#ActionRNNs.make_obs_list","page":"General Documentation","title":"ActionRNNs.make_obs_list","text":"make_obs_list\n\nMakes the obs list and initial state used for recurrent networks in an agent.  Uses an init function to define the init tuple.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.build_new_feat","page":"General Documentation","title":"ActionRNNs.build_new_feat","text":"build_new_feat(agent, state, action)\n\nconvenience for building new feature vector\n\n\n\n\n\n","category":"function"},{"location":"library/#Hidden-state-manipulation","page":"General Documentation","title":"Hidden state manipulation","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.HSStale\nActionRNNs.HSMinimize\nActionRNNs.HSRefil\nActionRNNs.get_hs_replay_strategy\nActionRNNs.modify_hs_in_er!\nActionRNNs.modify_hs_in_er_by_grad!\nActionRNNs.reset!","category":"page"},{"location":"library/#ActionRNNs.HSStale","page":"General Documentation","title":"ActionRNNs.HSStale","text":"HSStale\n\n\n\n\n\n","category":"type"},{"location":"library/#ActionRNNs.HSMinimize","page":"General Documentation","title":"ActionRNNs.HSMinimize","text":"HSMinimize\n\n\n\n\n\n","category":"type"},{"location":"library/#ActionRNNs.HSRefil","page":"General Documentation","title":"ActionRNNs.HSRefil","text":"HSRefil\n\n\n\n\n\n","category":"type"},{"location":"library/#ActionRNNs.get_hs_replay_strategy","page":"General Documentation","title":"ActionRNNs.get_hs_replay_strategy","text":"get_hs_replay_strategy(agent::AbstractERAgent)\n\nGet the replay strategy of the agent.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.modify_hs_in_er!","page":"General Documentation","title":"ActionRNNs.modify_hs_in_er!","text":"modify_hs_in_er!(hs_strategy::Bool, args...; kwargs...)\n\nLegacy function for hs_strategy as a boolean.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.modify_hs_in_er_by_grad!","page":"General Documentation","title":"ActionRNNs.modify_hs_in_er_by_grad!","text":"modify_hs_in_er!\n\nUpdating hidden state in the experience replay buffer.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.reset!","page":"General Documentation","title":"ActionRNNs.reset!","text":"reset!(m, h_init::Dict)\nreset!(m::Flux.Recur, h_init)\n\nReset the hidden state according to the dict hinit with keys from [`gethssymbollist`](@ref). If model is a recur just replace the hidden state. \n\n\n\n\n\n","category":"function"},{"location":"library/#Replay-buffer","page":"General Documentation","title":"Replay buffer","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.CircularBuffer\nActionRNNs.StateBuffer\nBase.length(buffer::ActionRNNs.CircularBuffer)\nBase.push!(buffer::CB, data::NamedTuple) where {CB<:ActionRNNs.CircularBuffer}\nActionRNNs.get_hs_details_for_er\nActionRNNs.hs_symbol_layer\nActionRNNs.get_hs_symbol_list\nActionRNNs.get_state_from_experience\nActionRNNs.get_information_from_experience\nActionRNNs.make_replay\nActionRNNs.get_hs_from_experience!\nActionRNNs.capacity\n","category":"page"},{"location":"library/#ActionRNNs.CircularBuffer","page":"General Documentation","title":"ActionRNNs.CircularBuffer","text":"CircularBuffer     Maintains a buffer of fixed size w/o reallocating and deallocating memory through a circular queue data struct.\n\n\n\n\n\n","category":"type"},{"location":"library/#ActionRNNs.StateBuffer","page":"General Documentation","title":"ActionRNNs.StateBuffer","text":"StateBuffer(size::Int, state_size)\n\nA cicular buffer for states. Typically used for images, can be used for state shapes up to 4d.\n\n\n\n\n\n","category":"type"},{"location":"library/#Base.length-Tuple{ActionRNNs.CircularBuffer}","page":"General Documentation","title":"Base.length","text":"length(buffer)\n\nReturns the current amount of data in the circular buffer. If the full flag is true then we return the size of the whole data frame.\n\n\n\n\n\n","category":"method"},{"location":"library/#Base.push!-Union{Tuple{CB}, Tuple{CB, NamedTuple}} where CB<:ActionRNNs.CircularBuffer","page":"General Documentation","title":"Base.push!","text":"push!(buffer, data)\n\nAdds data to the buffer, where data is an array of collections of types defined in CircularBuffer.datatypes returns row of data of added d\n\n\n\n\n\n","category":"method"},{"location":"library/#ActionRNNs.get_hs_details_for_er","page":"General Documentation","title":"ActionRNNs.get_hs_details_for_er","text":"get_hs_details_for_er(model)\n\nReturn the types, sizes, and symbols of the hidden state for the ER buffer.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.hs_symbol_layer","page":"General Documentation","title":"ActionRNNs.hs_symbol_layer","text":"hs_symbol_layer(l, idx)\n\nGet symbol of current layer's hidden state layer.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.get_hs_symbol_list","page":"General Documentation","title":"ActionRNNs.get_hs_symbol_list","text":"get_hs_symbol_list(model)\n\nGet list of hidden state symbols for all rnn layers.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.get_state_from_experience","page":"General Documentation","title":"ActionRNNs.get_state_from_experience","text":"get_state_from_experiment\n\nReturns hidden state from experience sampled from an experience replay buffer.  This assumes the replay has (:am1, :s, :a, :sp, :r, :t, :beg, hs_symbol...) as columns.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.get_information_from_experience","page":"General Documentation","title":"ActionRNNs.get_information_from_experience","text":"get_information_from_experience(agent, exp)\n\nGets the tuple of required details for the update of the agent. This is dispatched on the type of learning update.  You can use the helper abstract classes, or dispatch for your specific update.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.make_replay","page":"General Documentation","title":"ActionRNNs.make_replay","text":"make_replay\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.get_hs_from_experience!","page":"General Documentation","title":"ActionRNNs.get_hs_from_experience!","text":"get_hs_from_experience!(model, exp::NamedTuple, hs_dict::Dict, device)\nget_hs_from_experience!(model, exp::Vector, hs_dict::Dict, device)\n\nGet hs in the appropriate formate from the experience (either a Named Tuple or a vector of tuples Named Tuples).\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.capacity","page":"General Documentation","title":"ActionRNNs.capacity","text":"capacity(buffer)\nreturns the max number of elements the buffer can store.\n\n\n\n\n\n","category":"function"},{"location":"library/#Flux-Chain-Manipulation","page":"General Documentation","title":"Flux Chain Manipulation","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.contains_comp\nActionRNNs.find_layers_with_eq\nActionRNNs.find_layers_with_recur\nActionRNNs.contains_rnn_type\nActionRNNs.needs_action_input\nActionRNNs.contains_layer_type","category":"page"},{"location":"library/#ActionRNNs.contains_comp","page":"General Documentation","title":"ActionRNNs.contains_comp","text":"contains_comp(comp::Function, model)\n\nCheck if a layer of a model returns true with comp.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.find_layers_with_eq","page":"General Documentation","title":"ActionRNNs.find_layers_with_eq","text":"find_layers_with_eq(eq::Function, model)\n\nA function which takes a model and a function and returns the locations where the function returns true. This only supports composing chains twice.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.find_layers_with_recur","page":"General Documentation","title":"ActionRNNs.find_layers_with_recur","text":"find_layers_with_recur(model)\n\nFinds layers with recur. Uses find_layers_with_eq.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.contains_rnn_type","page":"General Documentation","title":"ActionRNNs.contains_rnn_type","text":"contains_rnn_type(m, rnn_type)\n\nChecks if the model has a specific rnn type.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.needs_action_input","page":"General Documentation","title":"ActionRNNs.needs_action_input","text":"needs_action_input(m)\n\nChecks if the model needs action input as a tuple.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.contains_layer_type","page":"General Documentation","title":"ActionRNNs.contains_layer_type","text":"contains_layer_type(model, type)\n\nCheck if the model has a specific layer type.\n\n\n\n\n\n","category":"function"},{"location":"library/#Policies","page":"General Documentation","title":"Policies","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.ϵGreedy\nActionRNNs.ϵGreedyDecay\nActionRNNs.get_prob\nActionRNNs.sample","category":"page"},{"location":"library/#ActionRNNs.ϵGreedy","page":"General Documentation","title":"ActionRNNs.ϵGreedy","text":"ϵGreedy(ϵ, action_set)\nϵGreedy(ϵ, num_actions)\n\nSimple ϵGreedy value policy.\n\n\n\n\n\n","category":"type"},{"location":"library/#ActionRNNs.ϵGreedyDecay","page":"General Documentation","title":"ActionRNNs.ϵGreedyDecay","text":"ϵGreedyDecay{AS}(ϵ_range, decay_period, warmup_steps, action_set::AS)\nϵGreedyDecay(ϵ_range, end_step, num_actions)\n\nThis is an acting policy which decays exploration linearly over time. This api will possibly change overtime once I figure out a better way to specify decaying epsilon.\n\nArguments\n\nϵ_range::Tuple{Float64, Float64}: (max epsilon, min epsilon) decay_period::Int: period epsilon decays warmup_steps::Int: number of steps before decay starts\n\n\n\n\n\n","category":"type"},{"location":"library/#ActionRNNs.get_prob","page":"General Documentation","title":"ActionRNNs.get_prob","text":"get_prob(ap::ϵGreedy, values, action)\n\nGet probabiliyt of action according to values.\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.sample","page":"General Documentation","title":"ActionRNNs.sample","text":"sample(ap::ϵGreedy, values, rng)\n\nSelect an action according to the values.\n\n\n\n\n\n","category":"function"},{"location":"library/#Feature-Constructors","page":"General Documentation","title":"Feature Constructors","text":"","category":"section"},{"location":"library/#Environments","page":"General Documentation","title":"Environments","text":"","category":"section"},{"location":"library/#RingWorld","page":"General Documentation","title":"RingWorld","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.RingWorld","category":"page"},{"location":"library/#ActionRNNs.RingWorld","page":"General Documentation","title":"ActionRNNs.RingWorld","text":"RingWorld States: 1     2     3     ...     n Vis:    1 <-> 0 <-> 0 <-> ... <-> 0 <-|         ^––––––––––––––-|\n\nchain_length: size (diameter) of ring actions: Forward of Backward\n\n\n\n\n\n","category":"type"},{"location":"library/#LinkedChains","page":"General Documentation","title":"LinkedChains","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.LinkedChainsV2","category":"page"},{"location":"library/#ActionRNNs.LinkedChainsV2","page":"General Documentation","title":"ActionRNNs.LinkedChainsV2","text":"LinkedChains\n\ntermmode:\n\nCONT: No termination\nTERM: Terminate after chain\n\ndynmode: \n\nSTRAIGHT: high Negative reward on wrong actions, but still progress through chain\nJUMP: Jump to different chain on wrong action\nSTUCK: Don't progress on wrong action\nJUMPSTUCK: Get \"lost\" with wrong actions, still being implemented.\n\n\n\n\n\n","category":"type"},{"location":"library/#TMaze","page":"General Documentation","title":"TMaze","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.TMaze","category":"page"},{"location":"library/#ActionRNNs.TMaze","page":"General Documentation","title":"ActionRNNs.TMaze","text":"TMaze\n\nTMaze as defined by Bram Bakker.\n\n\n\n\n\n","category":"type"},{"location":"library/#DirectionalTMaze","page":"General Documentation","title":"DirectionalTMaze","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.DirectionalTMaze","category":"page"},{"location":"library/#ActionRNNs.DirectionalTMaze","page":"General Documentation","title":"ActionRNNs.DirectionalTMaze","text":"DirectionalTMaze\n\nSimilar to ActionRNNs.TMaze but with a directional componenet overlayed ontop. This also changes to  observation structure, where the agent must know what direction it is facing to get information about which goal is the good goal.\n\n\n\n\n\n","category":"type"},{"location":"library/#Masked-Grid-World","page":"General Documentation","title":"Masked Grid World","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.MaskedGridWorld\nActionRNNs.MaskedGridWorldHelpers","category":"page"},{"location":"library/#ActionRNNs.MaskedGridWorld","page":"General Documentation","title":"ActionRNNs.MaskedGridWorld","text":"MaskedGridWorld\n\nThis grid world gives observations on a random number of states which are aliased (or not given obsstrategy). This environment also has the pacmanwrapping flag which makes it so the edges wrap around.\n\nwidth::Int: width of gw\nheight::Int: height of gw\nanchors::Int: number of anchors (Int), or list of anchor states\ngoals_or_rews: number of goals, list of goals, or list of rewards.\nobs_strategy: what obs are returned, :seperate, :full, aliased\npacman_wrapping::Bool: whether the walls are invisible and wrap around\n\n\n\n\n\n","category":"type"},{"location":"library/#ActionRNNs.MaskedGridWorldHelpers","page":"General Documentation","title":"ActionRNNs.MaskedGridWorldHelpers","text":"MaskedGridWorldHelpers\n\nHelper functions for the Masked grid world environment.\n\n\n\n\n\n","category":"module"},{"location":"library/#Lunar-Lander","page":"General Documentation","title":"Lunar Lander","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.LunarLander","category":"page"},{"location":"library/#ActionRNNs.LunarLander","page":"General Documentation","title":"ActionRNNs.LunarLander","text":"LunarLander\n\n\n\n\n\n","category":"type"},{"location":"library/#FluxUtils-Stuff","page":"General Documentation","title":"FluxUtils Stuff","text":"","category":"section"},{"location":"library/","page":"General Documentation","title":"General Documentation","text":"ActionRNNs.ExpUtils.FluxUtils.get_optimizer\nActionRNNs.ExpUtils.FluxUtils.RMSPropTF\nActionRNNs.ExpUtils.FluxUtils.RMSPropTFCentered","category":"page"},{"location":"library/#ActionRNNs.ExpUtils.FluxUtils.get_optimizer","page":"General Documentation","title":"ActionRNNs.ExpUtils.FluxUtils.get_optimizer","text":"get_optimizer\n\nReturn the Flux optimizer given a config dictionary. The optimizer name is found at key \"opt\". The parameters also change based on the  optimizer. \n\nOneParamInit: eta::Float\nTwoParamInit: eta::Float, rho::Float\nAdamParamInit: eta::Float, beta::Vector or (beta_m::Int, beta_v::Int)\n\n\n\n\n\n","category":"function"},{"location":"library/#ActionRNNs.ExpUtils.FluxUtils.RMSPropTF","page":"General Documentation","title":"ActionRNNs.ExpUtils.FluxUtils.RMSPropTF","text":"RMSPropTF(η, ρ)\n\nImplements the RMSProp algortihm as implemented in tensorflow. \n\nLearning Rate (η): Defaults to 0.001.\nRho (ρ): Defaults to 0.9.\nGamma (γ): Defaults to 0.0.\nEpsilon (ϵ): Defaults to 1e-6\n\nExamples\n\nReferences\n\nRMSProp Tensorflow RMSProp\n\n\n\n\n\n","category":"type"},{"location":"library/#ActionRNNs.ExpUtils.FluxUtils.RMSPropTFCentered","page":"General Documentation","title":"ActionRNNs.ExpUtils.FluxUtils.RMSPropTFCentered","text":"RMSPropTFCentered(η, ρ)\n\nImplements the Centered version of RMSProp algortihm as implemented in tensorflow. \n\nLearning Rate (η): Defaults to 0.001.\nRho (ρ): Defaults to 0.9.\nGamma (γ): Defaults to 0.0.\nEpsilon (ϵ): Defaults to 1e-6\n\nExamples\n\nReferences\n\nRMSProp Tensorflow RMSProp\n\n\n\n\n\n","category":"type"},{"location":"library/#Misc","page":"General Documentation","title":"Misc","text":"","category":"section"},{"location":"experiments/directional_tmaze/#Directional-TMaze","page":"Directional TMaze","title":"Directional TMaze","text":"","category":"section"},{"location":"experiments/directional_tmaze/#Experience-Replay-Experiment","page":"Directional TMaze","title":"Experience Replay Experiment","text":"","category":"section"},{"location":"experiments/directional_tmaze/","page":"Directional TMaze","title":"Directional TMaze","text":"DirectionalTMazeERExperiment\nDirectionalTMazeERExperiment.main_experiment\nDirectionalTMazeERExperiment.working_experiment\nDirectionalTMazeERExperiment.default_config\nDirectionalTMazeERExperiment.get_ann_size\nDirectionalTMazeERExperiment.construct_agent\nDirectionalTMazeERExperiment.construct_env","category":"page"},{"location":"experiments/directional_tmaze/#DirectionalTMazeERExperiment","page":"Directional TMaze","title":"DirectionalTMazeERExperiment","text":"DirectionalTMazeERExperiment\n\nAn experiment to compare different RNN cells using the ActionRNNs.DirectionalTMaze environment.\n\nUsage is detailed through the docs for \n\nDirectionalTMazeERExperiment.default_config\nDirectionalTMazeERExperiment.main_experiment\nDirectionalTMazeERExperiment.working_experiment\nDirectionalTMazeERExperiment.construct_env\nDirectionalTMazeERExperiment.construct_agent\n\n\n\n\n\n","category":"module"},{"location":"experiments/directional_tmaze/#DirectionalTMazeERExperiment.main_experiment","page":"Directional TMaze","title":"DirectionalTMazeERExperiment.main_experiment","text":"main_experiment\n\nRun an experiment from config. See DirectionalTMazeERExperiment.working_experiment  for details on running on the command line and DirectionalTMazeERExperiment.default_config  for info about the default configuration.\n\n\n\n\n\n","category":"function"},{"location":"experiments/directional_tmaze/#DirectionalTMazeERExperiment.working_experiment","page":"Directional TMaze","title":"DirectionalTMazeERExperiment.working_experiment","text":"working_experiment\n\nCreates a wrapper experiment where the main experiment is called with progress=true, testing=true  and the config is the default_config with the addition of the keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"experiments/directional_tmaze/#DirectionalTMazeERExperiment.default_config","page":"Directional TMaze","title":"DirectionalTMazeERExperiment.default_config","text":"Automatically generated docs for DirectionalTMazeERExperiment config.\n\nExperiment details.\n\nseed::Int: seed of RNG\nsteps::Int: Number of steps taken in the experiment\n\nLogging Extras\n\nBy default the experiment will log and save (depending on the synopsis flag) the logging group :EXP.  You can add extra logging groups and [group, name] pairs using the below arguments. Everything  added to save_extras will be passed to the save operation, and will be logged automatically. The  groups and names added to log_extras will be ommited from save_results but still passed back to the user through the data dict.\n\n<log_extras::Vector{Union{String, Vector{String}}>: which group and <name> to log to the data dict. This will not be passed to save.\n<save_extras::Vector{Union{String, Vector{String}}>: which groups and <names> to log to the data dict. This will be passed to save.\n\nEnvironment details\n\nThis experiment uses the DirectionalTMaze environment. The usable args are:\n\nsize::Int: Size of the hallway in directional tmaze.\n\nagent details\n\nRNN\n\nThe RNN used for this experiment and its total hidden size,  as well as a flag to use (or not use) zhu's deep  action network. See \n\ncell::String: The typeof cell. Many types are possible.\ndeepaction::Bool: Whether to use Zhu et. al.'s deep action 4 RNNs idea.   -internal_a::Int: the size of the action representation layer when deepaction=true\nnumhidden::Int:  Size of hidden state in RNNs.\n\nOptimizer details\n\nFlux optimizers are used. See flux documentation and ExpUtils.Flux.get_optimizer for details.\n\nopt::String: The name of the optimizer used\nParameters defined by the particular optimizer.\n\nLearning update and replay details including:\n\nReplay: \nreplay_size::Int: How many transitions are stored in the replay.\nwarm_up::Int: How many steps for warm-up (i.e. before learning begins).\nUpdate details: \nlupdate::String: Learning update name\ngamma::Float: the discount for learning update.\nbatch_size::Int: size of batch\ntruncation::Int: Length of sequences used for training.\nupdate_wait::Int: Time between updates (counted in agent interactions)\ntarget_update_wait::Int: Time between target network updates (counted in agent interactions)\nhs_strategy::String: Strategy for dealing w/ hidden state in buffer.\n\nDefault performance:\n\nTime: 0:02:28\n  episode:    5385\n  successes:  0.8351648351648352\n  loss:       1.0\n  l1:         0.0\n  action:     2\n  preds:      Float32[0.369189, 0.48326853, 0.993273]\n\n\n\n\n\n","category":"function"},{"location":"experiments/directional_tmaze/#DirectionalTMazeERExperiment.get_ann_size","page":"Directional TMaze","title":"DirectionalTMazeERExperiment.get_ann_size","text":"get_ann_size\n\nHelper function which constructs the environment and agent using default config and kwargs then returns the number of parameters in the model.\n\n\n\n\n\n","category":"function"},{"location":"experiments/directional_tmaze/#DirectionalTMazeERExperiment.construct_agent","page":"Directional TMaze","title":"DirectionalTMazeERExperiment.construct_agent","text":"construct_agent\n\nConstruct the agent for DirectionalTMazeERExperiment. See \n\n\n\n\n\n","category":"function"},{"location":"experiments/directional_tmaze/#DirectionalTMazeERExperiment.construct_env","page":"Directional TMaze","title":"DirectionalTMazeERExperiment.construct_env","text":"construct_env\n\nConstruct direction tmaze using:\n\nsize::Int size of hallway.\n\n\n\n\n\n","category":"function"},{"location":"experiments/directional_tmaze/#Intervention-Experiment-(Section-6)","page":"Directional TMaze","title":"Intervention Experiment (Section 6)","text":"","category":"section"},{"location":"experiments/directional_tmaze/","page":"Directional TMaze","title":"Directional TMaze","text":"DirectionalTMazeInterventionExperiment\nDirectionalTMazeInterventionExperiment.main_experiment\nDirectionalTMazeInterventionExperiment.working_experiment\nDirectionalTMazeInterventionExperiment.default_config\nDirectionalTMazeInterventionExperiment.get_ann_size\nDirectionalTMazeInterventionExperiment.construct_agent\nDirectionalTMazeInterventionExperiment.construct_env","category":"page"},{"location":"experiments/directional_tmaze/#DirectionalTMazeInterventionExperiment","page":"Directional TMaze","title":"DirectionalTMazeInterventionExperiment","text":"DirectionalTMazeInterventionExperiment\n\nAn experiment to compare different RNN cells using the ActionRNNs.DirectionalTMaze environment.\n\nUsage is detailed through the docs for \n\nDirectionalTMazeInterventionExperiment.default_config\nDirectionalTMazeInterventionExperiment.main_experiment\nDirectionalTMazeInterventionExperiment.working_experiment\nDirectionalTMazeInterventionExperiment.construct_env\nDirectionalTMazeInterventionExperiment.construct_agent\n\n\n\n\n\n","category":"module"},{"location":"experiments/directional_tmaze/#DirectionalTMazeInterventionExperiment.main_experiment","page":"Directional TMaze","title":"DirectionalTMazeInterventionExperiment.main_experiment","text":"main_experiment\n\nRun an experiment from config. See DirectionalTMazeInterventionExperiment.working_experiment  for details on running on the command line and DirectionalTMazeInterventionExperiment.default_config  for info about the default configuration.\n\n\n\n\n\n","category":"function"},{"location":"experiments/directional_tmaze/#DirectionalTMazeInterventionExperiment.working_experiment","page":"Directional TMaze","title":"DirectionalTMazeInterventionExperiment.working_experiment","text":"working_experiment\n\nCreates a wrapper experiment where the main experiment is called with progress=true, testing=true  and the config is the default_config with the addition of the keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"experiments/directional_tmaze/#DirectionalTMazeInterventionExperiment.default_config","page":"Directional TMaze","title":"DirectionalTMazeInterventionExperiment.default_config","text":"Automatically generated docs for DirectionalTMazeInterventionExperiment config.\n\nExperiment details.\n\nseed::Int: seed of RNG\nsteps::Int: Number of steps taken in the experiment\n\nLogging Extras\n\nBy default the experiment will log and save (depending on the synopsis flag) the logging group :EXP.  You can add extra logging groups and [group, name] pairs using the below arguments. Everything  added to save_extras will be passed to the save operation, and will be logged automatically. The  groups and names added to log_extras will be ommited from save_results but still passed back to the user through the data dict.\n\n<log_extras::Vector{Union{String, Vector{String}}>: which group and <name> to log to the data dict. This will not be passed to save.\n<save_extras::Vector{Union{String, Vector{String}}>: which groups and <names> to log to the data dict. This will be passed to save.\n\nEnvironment details\n\nThis experiment uses the DirectionalTMaze environment. The usable args are:\n\nsize::Int: Size of the hallway in directional tmaze.\n\nagent details\n\nRNN\n\nThe RNN used for this experiment and its total hidden size,  as well as a flag to use (or not use) zhu's deep  action network. See \n\ncell::String: The typeof cell. Many types are possible.\ndeepaction::Bool: Whether to use Zhu et. al.'s deep action 4 RNNs idea.   -internal_a::Int: the size of the action representation layer when deepaction=true\nnumhidden::Int:  Size of hidden state in RNNs.\n\nOptimizer details\n\nFlux optimizers are used. See flux documentation and ExpUtils.Flux.get_optimizer for details.\n\nopt::String: The name of the optimizer used\nParameters defined by the particular optimizer.\n\nLearning update and replay details including:\n\nReplay: \nreplay_size::Int: How many transitions are stored in the replay.\nwarm_up::Int: How many steps for warm-up (i.e. before learning begins).\nUpdate details: \nlupdate::String: Learning update name\ngamma::Float: the discount for learning update.\nbatch_size::Int: size of batch\ntruncation::Int: Length of sequences used for training.\nupdate_wait::Int: Time between updates (counted in agent interactions)\ntarget_update_wait::Int: Time between target network updates (counted in agent interactions)\nhs_strategy::String: Strategy for dealing w/ hidden state in buffer.\nIntervention details:\ninter_list::String: Points to the constructor fot the list of interventions to run on the agent after training.\n\\\"DTMazeV1\\\": Testing the start intervention and middle intervention.\ninter_freeze_training::Bool: whether to pause training when performing interventions.\ninter_num_episodes::Int: Number of episodes to perform interventions (these are after training episodes).\n\n\n\n\n\n","category":"function"},{"location":"experiments/directional_tmaze/#DirectionalTMazeInterventionExperiment.get_ann_size","page":"Directional TMaze","title":"DirectionalTMazeInterventionExperiment.get_ann_size","text":"get_ann_size\n\nHelper function which constructs the environment and agent using default config and kwargs then returns the number of parameters in the model.\n\n\n\n\n\n","category":"function"},{"location":"experiments/directional_tmaze/#DirectionalTMazeInterventionExperiment.construct_agent","page":"Directional TMaze","title":"DirectionalTMazeInterventionExperiment.construct_agent","text":"construct_agent\n\nConstruct the agent for DirectionalTMazeInterventionExperiment. See \n\n\n\n\n\n","category":"function"},{"location":"experiments/directional_tmaze/#DirectionalTMazeInterventionExperiment.construct_env","page":"Directional TMaze","title":"DirectionalTMazeInterventionExperiment.construct_env","text":"construct_env\n\nConstruct direction tmaze using:\n\nsize::Int size of hallway.\n\n\n\n\n\n","category":"function"},{"location":"experiments/lunarlander/#Lunar-Lander","page":"Lunar Lander","title":"Lunar Lander","text":"","category":"section"},{"location":"experiments/lunarlander/","page":"Lunar Lander","title":"Lunar Lander","text":"LunarLanderExperiment\nLunarLanderExperiment.main_experiment\nLunarLanderExperiment.working_experiment\nLunarLanderExperiment.default_config\nLunarLanderExperiment.get_ann_size\nLunarLanderExperiment.construct_agent\nLunarLanderExperiment.construct_env","category":"page"},{"location":"experiments/lunarlander/#LunarLanderExperiment","page":"Lunar Lander","title":"LunarLanderExperiment","text":"LunarLanderExperiment\n\nExperiment module for running experiments in Lunar Lander.\n\n\n\n\n\n","category":"module"},{"location":"experiments/lunarlander/#LunarLanderExperiment.main_experiment","page":"Lunar Lander","title":"LunarLanderExperiment.main_experiment","text":"main_experiment\n\nRun an experiment from config. See LunarLanderExperiment.working_experiment  for details on running on the command line and LunarLanderExperiment.default_config  for info about the default configuration.\n\n\n\n\n\n","category":"function"},{"location":"experiments/lunarlander/#LunarLanderExperiment.working_experiment","page":"Lunar Lander","title":"LunarLanderExperiment.working_experiment","text":"working_experiment\n\nCreates a wrapper experiment where the main experiment is called with progress=true, testing=true  and the config is the default_config with the addition of the keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"experiments/lunarlander/#LunarLanderExperiment.default_config","page":"Lunar Lander","title":"LunarLanderExperiment.default_config","text":"Automatically generated docs for LunarLanderExperiment config.\n\nExperiment details.\n\nseed::Int: seed of RNG\nsteps::Int: Number of steps taken in the experiment\n\nLogging Extras\n\nBy default the experiment will log and save (depending on the synopsis flag) the logging group :EXP.  You can add extra logging groups and [group, name] pairs using the below arguments. Everything  added to save_extras will be passed to the save operation, and will be logged automatically. The  groups and names added to log_extras will be ommited from save_results but still passed back to the user through the data dict.\n\n<log_extras::Vector{Union{String, Vector{String}}>: which group and <name> to log to the data dict. This will not be passed to save.\n<save_extras::Vector{Union{String, Vector{String}}>: which groups and <names> to log to the data dict. This will be passed to save.\n\nEnvironment details\n\nThis experiment uses the DirectionalTMaze environment. The usable args are:\n\nsize::Int: Size of the hallway in directional tmaze.\n\nagent details\n\nRNN\n\nThe RNN used for this experiment and its total hidden size,  as well as a flag to use (or not use) zhu's deep  action network. See \n\ncell::String: The typeof cell. Many types are possible.\ndeepaction::Bool: Whether to use Zhu et. al.'s deep action 4 RNNs idea.   -internal_a::Int: the size of the action representation layer when deepaction=true\nnumhidden::Int:  Size of hidden state in RNNs.\n\nOptimizer details\n\nFlux optimizers are used. See flux documentation and ExpUtils.Flux.get_optimizer for details.\n\nopt::String: The name of the optimizer used\nParameters defined by the particular optimizer.\n\nLearning update and replay details including:\n\nReplay: \nreplay_size::Int: How many transitions are stored in the replay.\nwarm_up::Int: How many steps for warm-up (i.e. before learning begins).\nUpdate details: \nlupdate::String: Learning update name\ngamma::Float: the discount for learning update.\nbatch_size::Int: size of batch\ntruncation::Int: Length of sequences used for training.\nupdate_wait::Int: Time between updates (counted in agent interactions)\ntarget_update_wait::Int: Time between target network updates (counted in agent interactions)\nhs_strategy::String: Strategy for dealing w/ hidden state in buffer.\n\nDefault Performance\n\nTime: 0:01:08\n  episode:     100\n  total_rews:  -249.31464\n  loss:        146.8288\n  l1:          0.6579351\n  action:      4\n  preds:       Float32[-18.375952, -17.984146, -17.7312, -17.009594]\n\n\n\n\n\n","category":"function"},{"location":"experiments/lunarlander/#LunarLanderExperiment.get_ann_size","page":"Lunar Lander","title":"LunarLanderExperiment.get_ann_size","text":"get_ann_size\n\nHelper function which constructs the environment and agent using default config and kwargs then returns the number of parameters in the model.\n\n\n\n\n\n","category":"function"},{"location":"experiments/lunarlander/#LunarLanderExperiment.construct_agent","page":"Lunar Lander","title":"LunarLanderExperiment.construct_agent","text":"construct_agent\n\nConstruct the agent for lunar lander.\n\n\n\n\n\n","category":"function"},{"location":"experiments/lunarlander/#LunarLanderExperiment.construct_env","page":"Lunar Lander","title":"LunarLanderExperiment.construct_env","text":"construct_environment\n\n\n\n\n\n","category":"function"},{"location":"#ActionRNNs.jl","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"","category":"section"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"Documentation for ActionRNNs.jl. See the right for more details on all the different aspects of the project.","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"General Documentation: All the library code necessary for building deep recurrent Q agents.\nExperiments: A non-exhaustive list of experiments, and some tips on running these experiments.","category":"page"},{"location":"#Setup","page":"ActionRNNs.jl","title":"Setup","text":"","category":"section"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"Clone repository locally, install Julia v1.7.x. In the ActionRNN folder.","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"Start the julia repl with julia","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"julia> ]add Revise, Plots\njulia> ]activate .\njulia> ]instantiate","category":"page"},{"location":"#To-run-the-example-experiment","page":"ActionRNNs.jl","title":"To run the example experiment","text":"","category":"section"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"julia> using Revise; includet(\"experiment/RingWorldERExperiment.jl\")\njulia> ret = RingWorldERExperiment.working_experiment()","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"This should run a Ring World experiment with the action RNN. This return a dictionary containing predictions and errors. You should easily be able to analyze the error and see relatively good performance here.","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"To analyze the data:","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"julia> using Statistics\njulia> rmse = sqrt.(mean(ret[\"err\"].^2; dims=2))\njulia> mean(rmse)   # You should get 0.006968971...","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"To plot:","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"julia> using Plots\njulia> gr()\njulia> plot(rmse) # this will plot all the data points, and will be noisy\njulia> plot(mean(reshape(rmse, 1000, 300); dims=1)') #this will plot a windowed average of points","category":"page"},{"location":"#Consistency-Tests","page":"ActionRNNs.jl","title":"Consistency Tests","text":"","category":"section"},{"location":"#Adding-Consistency-Tests","page":"ActionRNNs.jl","title":"Adding Consistency Tests","text":"","category":"section"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"Adding a consistency test requires you know which experiment you are targeting, what cell, and the various arguments you want. For example. There is some magic macros in the ActionRNNsTests to make this simpler. Bellow is the easiest way to find the test for an experiment with an example in ringworld_er.  This workflow is still WIP, but this should get you all you need to add new cells to the files in tests/consistency.","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"push!(LOAD_PATH, \"../ActionRNNs.jl/test/\")\nusing ReTest\nimport ActionRNNsTests: ActionRNNsTests, Consistency.@run_experiment\nbegin\n    ret = @run_experiment RingWorldERExperiment \"FacMARNN\" Consistency.RINGWORLD_ER_BASE_CONFIG factors=3\n    sum(ret[\"err\"])\nend","category":"page"},{"location":"#Running-Consistency-Tests","page":"ActionRNNs.jl","title":"Running Consistency Tests","text":"","category":"section"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"We use ReTest to run all our tests. This allows for testing certain experiments or cells across all experiments.","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"push!(LOAD_PATH, \"../ActionRNNs.jl/test/\")\nusing ReTest; import ActionRNNsTests\nretest() # Run all tests\nretest(\"AARNN\") # Run all AARNN tests\nretest(\"Ringworld\") # Run all Ring World experiments\nretest(\"Fac\") # run all factored tests (including FacMA* and FacTucMA*)","category":"page"},{"location":"#Documentation","page":"ActionRNNs.jl","title":"Documentation","text":"","category":"section"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"We are using Documenter.jl. Please see the documentation for documenter to figure out how it works. These docs are very much WIP, but for now we are putting all the docs in library.md raw and the index.md will have a bit more details to walk ppl through the different ideas in the repo.","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"To add docs all you need to do is add a documentation section above a function or struct and then add this to the appropriate docs section.","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"To build the documentation go to the ActionRNNs/docs directory and run:","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"$ julia --project make.jl","category":"page"},{"location":"","page":"ActionRNNs.jl","title":"ActionRNNs.jl","text":"I'm still figuring out how to best host the docs.","category":"page"},{"location":"experiments/masked_gw/#Masked-Grid-World","page":"Masked Grid World","title":"Masked Grid World","text":"","category":"section"},{"location":"experiments/masked_gw/#Experience-Replay-Experiment","page":"Masked Grid World","title":"Experience Replay Experiment","text":"","category":"section"},{"location":"experiments/masked_gw/","page":"Masked Grid World","title":"Masked Grid World","text":"MaskedGridWorldERExperiment\nMaskedGridWorldERExperiment.main_experiment\nMaskedGridWorldERExperiment.working_experiment\nMaskedGridWorldERExperiment.default_config\nMaskedGridWorldERExperiment.construct_agent\nMaskedGridWorldERExperiment.construct_env\nMaskedGridWorldERExperiment.get_ann_size","category":"page"},{"location":"experiments/masked_gw/#MaskedGridWorldERExperiment","page":"Masked Grid World","title":"MaskedGridWorldERExperiment","text":"MaskedGridWorldERExperiment\n\nModule for running a standard experiment in masked grid world. An experiment to compare different RNN cells using the ActionRNNs.MaskedGridWorld environment.\n\nUsage is detailed through the docs for \n\nMaskedGridWorldERExperiment.default_config\nMaskedGridWorldERExperiment.main_experiment\nMaskedGridWorldERExperiment.working_experiment\nMaskedGridWorldERExperiment.construct_env\nMaskedGridWorldERExperiment.construct_agent\n\n\n\n\n\n","category":"module"},{"location":"experiments/masked_gw/#MaskedGridWorldERExperiment.main_experiment","page":"Masked Grid World","title":"MaskedGridWorldERExperiment.main_experiment","text":"main_experiment\n\nRun an experiment from config. See MaskedGridWorldERExperiment.working_experiment  for details on running on the command line and MaskedGridWorldERExperiment.default_config  for info about the default configuration.\n\n\n\n\n\n","category":"function"},{"location":"experiments/masked_gw/#MaskedGridWorldERExperiment.working_experiment","page":"Masked Grid World","title":"MaskedGridWorldERExperiment.working_experiment","text":"working_experiment\n\nCreates a wrapper experiment where the main experiment is called with progress=true, testing=true  and the config is the default_config with the addition of the keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"experiments/masked_gw/#MaskedGridWorldERExperiment.default_config","page":"Masked Grid World","title":"MaskedGridWorldERExperiment.default_config","text":"Automatically generated docs for MaskedGridWorldERExperiment config.\n\nExperiment details.\n\nseed::Int: seed of RNG\nsteps::Int: Number of steps taken in the experiment\n\nEnvironment details\n\nThis experiment uses the MaskedGridWorldExperiment environment. The usable args are:\n\nwidth::Int, height::Int: Width and height of the grid world\nnum_anchors::Int: number of states with active observations\nnum_goals::Int: number of goals\n\nagent details\n\nRNN\n\nThe RNN used for this experiment and its total hidden size,  as well as a flag to use (or not use) zhu's deep  action network. See \n\ncell::String: The typeof cell. Many types are possible.\ndeepaction::Bool: Whether to use Zhu et. al.'s deep action 4 RNNs idea.\ninternal_a::Int: the size of the action representation layer when deepaction=true\nnumhidden::Int:  Size of hidden state in RNNs.\n\nOptimizer details\n\nFlux optimizers are used. See flux documentation and ExpUtils.Flux.get_optimizer for details.\n\nopt::String: The name of the optimizer used\nParameters defined by the particular optimizer.\n\nLearning update and replay details including:\n\nReplay: \nreplay_size::Int: How many transitions are stored in the replay.\nwarm_up::Int: How many steps for warm-up (i.e. before learning begins).\nUpdate details: \nlupdate_agg::String: the aggregation function for the QLearning update.\ngamma::Float: the discount for learning update.\nbatch_size::Int: size of batch\ntruncation::Int: Length of sequences used for training.\nupdate_wait::Int: Time between updates (counted in agent interactions)\ntarget_update_wait::Int: Time between target network updates (counted in agent interactions)\nhs_strategy::String: Strategy for dealing w/ hidden state in buffer.\n\n\n\n\n\n","category":"function"},{"location":"experiments/masked_gw/#MaskedGridWorldERExperiment.construct_agent","page":"Masked Grid World","title":"MaskedGridWorldERExperiment.construct_agent","text":"construct_agent\n\nConstruct the agent for MaskedGridWorldERExperiment. See \n\n\n\n\n\n","category":"function"},{"location":"experiments/masked_gw/#MaskedGridWorldERExperiment.construct_env","page":"Masked Grid World","title":"MaskedGridWorldERExperiment.construct_env","text":"construct_env\n\nConstruct MaskedGridWorld. settings\n\n\"width\": width of gridworld\n\"height\": height of gridworld\n\"num_anchors\": number of anchors\n\"num_goals\": number of goals\n\n\n\n\n\n","category":"function"},{"location":"experiments/masked_gw/#MaskedGridWorldERExperiment.get_ann_size","page":"Masked Grid World","title":"MaskedGridWorldERExperiment.get_ann_size","text":"get_ann_size\n\nHelper function which constructs the environment and agent using default config and kwargs then returns the number of parameters in the model.\n\n\n\n\n\n","category":"function"}]
}
